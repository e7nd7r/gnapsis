{
  "tasks": [
    {
      "id": "T1-architecture-layers",
      "category": "architecture",
      "difficulty": "medium",
      "prompt": "Describe the layered architecture of the gnapsis codebase. What are the main layers, what module does each layer correspond to, and what is the direction of dependencies between layers? Be specific about module names and file paths.",
      "expected_answer_keywords": [
        "mcp", "services", "repositories", "graph", "models",
        "dependency injection", "FromRef", "FromContext", "Context",
        "PostgresClient", "Apache AGE", "GraphClient",
        "Clean Architecture",
        "Repository Pattern",
        "separation of concerns",
        "AppError",
        "config",
        "cross-cutting",
        "compile-time"
      ],
      "rubric": "Tiered scoring:\n- 4-5: Identifies 3+ layers with correct dependency direction and specific module paths.\n- 6-7: Identifies 4+ layers, mentions dependency injection mechanism (Context/FromRef/FromContext), identifies PostgreSQL + Apache AGE as the backend.\n- 8: Names specific components per layer (individual repositories, services by name), identifies GraphClient as a key abstraction trait, recognizes the architectural style (Clean Architecture, Hexagonal, or Onion).\n- 9: Discusses cross-cutting concerns (error handling via AppError across layers, configuration management, models as a shared layer), explains compile-time DI via derive macros, identifies SOLID principles in practice (Dependency Inversion via traits, Single Responsibility per layer), mentions the Repository Pattern in the data access layer.\n- 10: Identifies 5+ layers with all key components named, notes observability patterns or their absence (logging, tracing), mentions standardization/coding conventions observed, identifies architectural trade-offs or potential improvements. Provides a comprehensive engineering analysis, not just a structural listing.",
      "max_turns": 15
    },
    {
      "id": "T2-dependency-trace",
      "category": "dependency_tracing",
      "difficulty": "hard",
      "prompt": "Trace the complete data flow when the MCP tool 'search' is called with a query string. Starting from the MCP tool handler, trace through every layer until the embedding is generated and the database query is executed. List each function call in order with its file path.",
      "expected_answer_keywords": [
        "mcp/tools", "search",
        "GraphService", "unified_search", "semantic_search",
        "embedder", "embed",
        "QueryRepository", "search_entities_by_embedding",
        "cosine_similarity",
        "fetch_all",
        "FromContext", "Context",
        "async", "await",
        "Result", "AppError",
        "Vec", "embedding",
        "Cypher"
      ],
      "rubric": "Tiered scoring:\n- 4-5: Gets 3+ hops correct in the call chain with file paths.\n- 6-7: Complete trace from MCP tool handler through GraphService to QueryRepository, mentions embedding vector generation step, reaches the database layer.\n- 8: Identifies data transformations at layer boundaries (query string -> embedding vector -> Cypher query -> parsed results), notes the DI mechanism that wires the layers together (FromContext/Context), identifies the Repository Pattern at the data access layer.\n- 9: Explains error propagation along the call chain (Result<T, AppError> threading, ? operator usage), identifies async/await patterns throughout the trace, notes how the embedding model is injected and abstracted.\n- 10: Discusses potential failure modes at each hop (network errors, embedding failures, query timeouts), notes absence or presence of observability (logging, tracing spans), identifies where input validation occurs, assesses the separation of concerns quality across the traced path.",
      "max_turns": 20
    },
    {
      "id": "T3-error-propagation",
      "category": "error_analysis",
      "difficulty": "medium",
      "prompt": "How does error handling work in this codebase? What error type does src/error.rs define, what variants does it have, and how are errors converted for MCP protocol responses? Show specific mappings from error variants to MCP error codes.",
      "expected_answer_keywords": [
        "AppError", "thiserror",
        "EntityNotFound", "CategoryNotFound", "ScopeNotFound",
        "InvalidBelongsTo", "Validation",
        "Connection", "Query",
        "ErrorData", "ErrorCode",
        "RESOURCE_NOT_FOUND", "INVALID_PARAMS",
        "From", "Into",
        "Result",
        "boundary",
        "user-facing",
        "internal"
      ],
      "rubric": "Tiered scoring:\n- 4-5: Identifies AppError enum and its use of thiserror derive macro, lists some variants.\n- 6-7: Lists 6+ variants, explains the From<AppError> for ErrorData conversion, provides at least 3 specific variant-to-error-code mappings.\n- 8: Identifies the error boundary pattern — internal errors (Connection, Query) vs domain errors (EntityNotFound, Validation) vs user-facing MCP errors (ErrorData). Explains how the conversion layer sanitizes internal details from external responses.\n- 9: Evaluates the error handling strategy against best practices — notes whether errors carry enough context for debugging, whether error chains preserve root causes, whether there is structured error logging. Identifies the Result<T, AppError> convention used consistently across layers.\n- 10: Assesses completeness of error handling — identifies any unhandled edge cases or places where errors are silently swallowed. Notes presence or absence of error recovery strategies (retries, fallbacks). Discusses whether the error taxonomy supports observability (correlation IDs, error categorization for metrics). Compares the approach to standard patterns (RFC 7807, error envelopes).",
      "max_turns": 10
    },
    {
      "id": "T4-impact-analysis",
      "category": "impact_analysis",
      "difficulty": "hard",
      "prompt": "If I wanted to add a new graph backend (say SQLite instead of PostgreSQL), what traits would I need to implement, what files would need to change, and what would NOT need to change? Analyze the abstraction boundaries in the codebase.",
      "expected_answer_keywords": [
        "CypherExecutor", "GraphClient", "Transaction", "SqlExecutor",
        "graph/traits.rs", "graph/backends",
        "Graph", "generic",
        "services would NOT change",
        "repositories would NOT change",
        "AppGraph", "context.rs",
        "Dependency Inversion",
        "Strategy Pattern",
        "type alias",
        "migration",
        "config"
      ],
      "rubric": "Tiered scoring:\n- 4-5: Identifies the key traits that need implementation (CypherExecutor, GraphClient, Transaction).\n- 6-7: Full trait list including SqlExecutor, identifies that a new file/module under graph/backends/ is needed, explains that services, repositories, and MCP tools would NOT change.\n- 8: Mentions the AppGraph type alias and Graph<C: GraphClient> generic wrapper, identifies this as the Dependency Inversion Principle in practice (upper layers depend on traits, not implementations), recognizes the Strategy Pattern enabling backend swapping.\n- 9: Discusses configuration changes needed (connection strings, backend selection in config), migration implications (schema compatibility, Cypher dialect differences), testing strategy for the new backend (trait-based testing, integration tests).\n- 10: Identifies potential issues with backend parity (feature gaps — e.g., SQLite lacking native graph extensions like AGE), discusses transaction semantics differences, notes where the abstraction might leak (backend-specific Cypher syntax, performance characteristics). Evaluates the overall quality of the abstraction boundary — what it enables and where it might break down.",
      "max_turns": 15
    },
    {
      "id": "T5-command-pattern",
      "category": "pattern_recognition",
      "difficulty": "medium",
      "prompt": "The file src/services/commands.rs implements a specific design pattern. Identify the pattern, explain how it works in this codebase, list all command variants, and describe the failure handling semantics.",
      "expected_answer_keywords": [
        "Command Pattern",
        "EntityCommand", "CommandService",
        "Attach", "Unattach", "Add", "Relate", "Unrelate", "Link", "Unlink",
        "CommandResult", "ExecutedCommand", "FailedCommand",
        "sequential", "stop on failure", "skipped",
        "encapsulation",
        "enum",
        "transaction"
      ],
      "rubric": "Tiered scoring:\n- 4-5: Identifies the Command Pattern by name, lists some variants.\n- 6-7: Lists all 7 EntityCommand variants (Attach, Unattach, Add, Relate, Unrelate, Link, Unlink), explains sequential execution with stop-on-first-failure semantics.\n- 8: Explains CommandResult structure with executed/failed/skipped fields, identifies how Rust enums are used to model the command variants (algebraic data types vs traditional OOP Command interface), notes that each variant carries its own data.\n- 9: Evaluates the pattern's fitness — discusses benefits (encapsulation of operations, batch execution, audit trail via results) and limitations. Identifies related patterns (Builder for command construction, possible Undo/Redo potential). Notes whether commands execute within a transaction boundary.\n- 10: Assesses engineering quality — error handling within individual commands, whether commands are idempotent, extensibility for new command types, observability of command execution (logging, metrics). Compares to alternative approaches (event sourcing, CQRS) and discusses trade-offs.",
      "max_turns": 10
    },
    {
      "id": "T6-bfs-algorithm",
      "category": "algorithm_understanding",
      "difficulty": "hard",
      "prompt": "Explain the Best-First Search algorithm implemented in src/services/graph.rs for semantic subgraph extraction. What data structures does it use, how does it score nodes, what are the two scoring strategies and how do they differ, and what are the budget constraints?",
      "expected_answer_keywords": [
        "BinaryHeap", "PQNode", "priority queue",
        "cosine_similarity", "query_embedding", "embedding",
        "Global", "BranchPenalty",
        "max_tokens", "max_nodes", "min_relevance",
        "TOKENS_PER_CHAR", "BRANCH_BUDGET",
        "CacheEntry", "entity_cache",
        "greedy",
        "token budget",
        "O(n log n)"
      ],
      "rubric": "Tiered scoring:\n- 4-5: Identifies BinaryHeap as the priority queue with PQNode entries, explains basic greedy traversal.\n- 6-7: Explains scoring using cosine similarity between entity embeddings and query embedding, names both strategies (Global, BranchPenalty), mentions the 3 budget constraints (max_tokens, max_nodes, min_relevance).\n- 8: Contrasts Global vs BranchPenalty in detail (Global uses only cumulative token consumption to decay scores; BranchPenalty additionally penalizes deep branches via per-branch token tracking to encourage breadth), explains the token estimation via TOKENS_PER_CHAR.\n- 9: Discusses entity caching (CacheEntry, avoiding redundant fetches), analyzes algorithm complexity (time and space), identifies the greedy nature and its implications (local optima, no backtracking). Notes how the algorithm handles graph cycles.\n- 10: Evaluates the algorithm design — discusses trade-offs of the two strategies for different query types, identifies potential improvements (beam search, A* with heuristics), assesses the token budget mechanism accuracy. Notes observability (logging of traversal decisions) and testability of the algorithm.",
      "max_turns": 15
    },
    {
      "id": "T7-find-duplication",
      "category": "code_quality",
      "difficulty": "hard",
      "prompt": "Find duplicated code in the gnapsis codebase. Focus on the src/repositories/ directory. Identify functions or patterns that are implemented more than once across different files, and explain what they do.",
      "expected_answer_keywords": [
        "row_to_entity", "EntityRepository", "QueryRepository",
        "row_to_code_reference", "DocumentRepository",
        "row_to_text_reference",
        "parse_scope", "CategoryRepository", "SchemaRepository",
        "cosine_similarity",
        "embedding", "f64", "f32",
        "DRY",
        "trait", "shared",
        "refactor"
      ],
      "rubric": "Tiered scoring:\n- 4-5: Identifies 2-3 duplications with file locations.\n- 6-7: Identifies 3-4 duplications with explanations of what each duplicated function does. Duplications include: (1) row_to_entity in entity.rs vs query.rs, (2) row_to_code_reference in document.rs vs query.rs, (3) row_to_text_reference in document.rs vs query.rs, (4) parse_scope in category.rs vs schema.rs, (5) cosine_similarity in query.rs vs graph.rs, (6) embedding serialization/deserialization patterns.\n- 8: Identifies 5+ duplications, explains the DRY violation and its maintenance risk, suggests concrete refactoring approach (shared trait, common module, helper functions).\n- 9: Analyzes the root cause of the duplication — identifies architectural reasons (repositories not sharing a common conversion layer, query.rs needing to duplicate entity/document parsing because it aggregates results across entity types). Discusses impact on maintainability and bug risk.\n- 10: Provides a refactoring plan with specific implementation details (e.g., extract a RowConversion trait, create a shared module). Evaluates the trade-offs of the refactoring (complexity vs DRY). Notes whether the duplications have already diverged (subtle differences between copies). Identifies whether tests cover the duplicated code paths and the risk of inconsistency.",
      "max_turns": 20
    }
  ]
}
